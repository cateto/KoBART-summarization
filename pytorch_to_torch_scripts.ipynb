{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.onnx import export\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import BartForConditionalGeneration\n",
    "from kobart import get_kobart_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /workspace/git/KoBART-summarization/.cache/kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_kobart_tokenizer()\n",
    "\n",
    "sentence_text = '요약을 위해 작성하는 글입니다. 사실 이 글의 요점은 아무 주제도 없다는 것에 있습니다. 부디 기계가 이 요점을 잘 정리해서 보여주었으면 좋겠습니다.'\n",
    "max_length = 1024\n",
    "\n",
    "model_input = tokenizer(\n",
    "            sentence_text,\n",
    "            return_tensors='np',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "input_ids = model_input[\"input_ids\"]\n",
    "attention_mask = model_input[\"attention_mask\"]\n",
    "\n",
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(attention_mask)\n",
    "\n",
    "class Pytorch_to_TorchScript(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Pytorch_to_TorchScript, self).__init__()\n",
    "    self.model = BartForConditionalGeneration.from_pretrained('./kobart_summary').cuda()\n",
    "  def forward(self, data, attention_mask=None):\n",
    "    return self.model.generate(data.cuda(), num_beams=4,  max_length=max_length,  eos_token_id=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:212: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:218: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:249: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:935: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_ids.shape[-1] >= max_length:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:182: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  self._done = torch.tensor([False for _ in range(batch_size)], dtype=torch.bool, device=self.device)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1767: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert (\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py:863: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_logits_process.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if cur_len < self.min_length:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_logits_process.py:573: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if cur_len == self.max_length - 1:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1839: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:217: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  assert batch_size == (input_ids.shape[0] // self.group_size)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:217: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert batch_size == (input_ids.shape[0] // self.group_size)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:225: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self._done[batch_idx]:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:241: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:245: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (eos_token_id is not None) and (next_token.item() == eos_token_id):\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:271: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self._done[batch_idx] = self._done[batch_idx] or beam_hyp.is_done(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:272: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  next_scores[batch_idx].max().item(), cur_len\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_utils.py:1866: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_stopping_criteria.py:108: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return any(criteria(input_ids, scores) for criteria in self)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:252: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  next_score.item(),\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:376: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.worst_score = min(score, self.worst_score)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:369: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if len(self) < self.num_beams or score > self.worst_score:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sorted_next_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.beams)])\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:297: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self._done[batch_idx]:\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:315: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sorted_hyps = sorted(beam_hyp.beams, key=lambda x: x[0])\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:320: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  sent_lengths[self.num_beam_hyps_to_keep * i + j] = len(best_hyp)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:327: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sent_max_len = min(sent_lengths.max().item() + 1, max_length)\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:330: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sent_lengths.min().item() != sent_lengths.max().item():\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation_beam_search.py:337: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sent_lengths[i] < max_length:\n"
     ]
    }
   ],
   "source": [
    "# Creating the trace\n",
    "pt_model = Pytorch_to_TorchScript().eval()\n",
    "traced_model = torch.jit.trace(pt_model, (tokens_tensor, segments_tensors))\n",
    "torch.jit.save(traced_model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Pytorch_to_TorchScript\n",
       "  (model): RecursiveScriptModule(\n",
       "    original_name=BartForConditionalGeneration\n",
       "    (model): RecursiveScriptModule(\n",
       "      original_name=BartModel\n",
       "      (shared): RecursiveScriptModule(original_name=Embedding)\n",
       "      (encoder): RecursiveScriptModule(\n",
       "        original_name=BartEncoder\n",
       "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "        (embed_positions): RecursiveScriptModule(original_name=BartLearnedPositionalEmbedding)\n",
       "        (layers): RecursiveScriptModule(\n",
       "          original_name=ModuleList\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=BartEncoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): RecursiveScriptModule(original_name=LayerNorm)\n",
       "      )\n",
       "      (decoder): RecursiveScriptModule(\n",
       "        original_name=BartDecoder\n",
       "        (embed_tokens): RecursiveScriptModule(original_name=Embedding)\n",
       "        (embed_positions): RecursiveScriptModule(original_name=BartLearnedPositionalEmbedding)\n",
       "        (layers): RecursiveScriptModule(\n",
       "          original_name=ModuleList\n",
       "          (0): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (1): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (2): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (3): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (4): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "          (5): RecursiveScriptModule(\n",
       "            original_name=BartDecoderLayer\n",
       "            (self_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (self_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (encoder_attn): RecursiveScriptModule(\n",
       "              original_name=BartAttention\n",
       "              (k_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (v_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (q_proj): RecursiveScriptModule(original_name=Linear)\n",
       "              (out_proj): RecursiveScriptModule(original_name=Linear)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "            (fc1): RecursiveScriptModule(original_name=Linear)\n",
       "            (fc2): RecursiveScriptModule(original_name=Linear)\n",
       "            (final_layer_norm): RecursiveScriptModule(original_name=LayerNorm)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): RecursiveScriptModule(original_name=LayerNorm)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): RecursiveScriptModule(original_name=Linear)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = torch.jit.load(\"model.pt\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /workspace/git/KoBART-summarization/.cache/kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_kobart_tokenizer()\n",
    "max_length = 1024\n",
    "\n",
    "sentence_text = \"\"\"‘왜 한국에서 유리천장을 뚫은 여성 최고경영자(CEO)와 임원 중 외국계 기업 출신이 많을까?’\n",
    "\n",
    "오랫동안 품은 의문이다. 통계적 증명이나 깊이 있는 연구 결과는 아니지만 한 가지 가설이 가능하다. 한국 기업과 외국 기업의 문화 차이, 좀 더 구체적으론 일과 가정의 양립이 제도적으로, 그리고 실질적으로 보장되느냐 여부가 아닐까 싶다. 출산과 육아가 커리어의 공백이 되는 한국. 이 문제가 한국이 초저출산 국가가 된 배경 중 하나임은 분명해 보인다.\n",
    "\n",
    "한국은 일론 머스크 테슬라 CEO까지 ‘인구 붕괴’를 걱정할 정도로 저출산 문제가 심각하다. 취업률 하락, 집값 폭등 등 경제적인 이유와 함께 가치관 변화, 특히 여성들의 인식 변화가 출산율 급감에 영향을 미쳤을 것이다. 출산율은 각종 대책에도 불구하고 2016년 이후 뚝뚝 떨어지고 있다. 이른바 MZ세대, 그중 M(밀레니얼)세대가 결혼 및 출산 시기에 접어드는 시점과 맞물린다.\n",
    "\n",
    "그들은 왜 출산을 기피할까. 그들의 엄마 세대는 공부도 할 만큼 했고, 사회생활을 하고 싶어 했다. 하지만 그중 많은 사람이 출산·육아 때문에 포기할 수밖에 없었다. 그런 엄마들로부터 딸들이 들었던 말은 이런 것이다. “너는 하고 싶은 거 하면서 살아라.” 아들들의 생각도 달라졌다. 아빠 세대의 나 홀로 부양 부담이 싫어졌다. 20대 절반은 결혼 후 아이 없이 사는 것에 동의한다는 설문조사 결과가 있다.\n",
    "\n",
    "그 결과가 대한민국 합계 출산율 0.81명이다. 우리나라 여성 한 명이 평생 낳을 것으로 예상되는 평균 출생아 수가 1명도 안 된다는 얘기다. 경제협력개발기구(OECD) 38개 회원국 가운데 합계출산율이 1명에도 못 미치는 나라는 한국이 유일하다. 지금 같은 추세면 50년 뒤엔 사회적으로 젊은 사람 1명이 노인 1명을 부양해야 한다. 저출산·고령화가 심각하다는 것엔 사회적 공감대가 형성돼 있다. 문제는 여러 상황이 원인이자 결과로 뒤얽혀 있어 해법 찾기가 쉽지 않다는 것이다.\n",
    "\n",
    "결혼과 출산은 지극히 사적인 영역이고, 개인의 선택이다. 당장 사람들의 생각과 가치관을 바꾸긴 쉽지 않다. 그렇기 때문에 저출산 대책은 마음은 있지만 여건 때문에 자녀 갖기를 망설이는 이들의 애로사항을 풀어주는 데서 시작해야 한다.\n",
    "\n",
    "모든 정책과 제도는 만드는 것만큼이나 정착을 위한 디테일이 중요하다. 저출산 대책도 마찬가지다. 아이돌봄 서비스 같은 경우 당장 필요할 때 이용하기 어렵다는 불만이 많다. 여전히 공급이 부족하고 어딘가에 허점이 있기 때문일 것이다. 교육부 보건복지부 여성가족부 등 각 부처 정책의 연계성 등을 종합적으로 들여다볼 필요가 있다.\n",
    "\n",
    "기업 문화가 달라져야 한다는 것은 직접적인 과제다. 저출산은 기업에도 남의 일이 아니다. 인재 풀과 시장이 쪼그라드는 문제다. 육아휴직과 연월차 휴가만 필요할 때 쓸 수 있어도 육아에 대한 부담이 훨씬 줄어들 것이다. 외국계 기업에 여성 임원이 많은 것은 우연이 아니다.\n",
    "\n",
    "특히 남성 육아휴직에 대한 인식이 달라져야 한다. 출생아 부모 가운데 육아휴직을 쓰는 남성 비중은 4.1%에 불과하다. 여성은 65.2%다. 남성 육아휴직의 확산은 여성들이 아이를 낳으면 육아는 결국 자신의 몫이 될 것이란 부담과 경력단절, 또는 동료보다 뒤처질 것이란 두려움을 완화시켜 줄 수 있을 것이다. 모든 일이 그렇듯 기업 최고경영자의 의지와 메시지가 중요하다. 정부도 기업 문화가 변할 수 있도록 강력한 인센티브를 줘야 한다.\n",
    "\n",
    "박성완 논설위원겸 경제교육연구소장 psw@hankyung.com\"\"\"\n",
    "model_input = tokenizer(\n",
    "            sentence_text,\n",
    "            return_tensors='np',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "input_ids = model_input[\"input_ids\"]\n",
    "attention_mask = model_input[\"attention_mask\"]\n",
    "\n",
    "tokens_tensor = torch.tensor(input_ids)\n",
    "segments_tensors = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = [tokens_tensor, segments_tensors]\n",
    "result = loaded_model(*dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국 기업과 외국 기업의 문화 차이, 좀 더 구체적으론 일과 가정의 양립이 제도적으로, 그리고 실질적으로 보장되느냐 여부가 한국 초저출산 국가가 된 배경 중 하나이며, 한국 기업과 외국 기업의 문화 차이, 좀 더 구체적으론 일과 가정의 양립이 제도적으로, 그리고 실질적으로 보장되느냐 여부가 아닐까 싶다. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n한국은 일론 머스크 테슬라 CEO까지 ‘인구 붕괴’를 걱정할 정도로 저출산 문제가 심각해 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(result.squeeze().tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
